{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML, or Automated Machine Learning, is the process of automating the end-to-end process of applying machine learning to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model, including:\n",
    "\n",
    "1. **Data Preprocessing**: Handling missing values, encoding categorical variables, feature scaling, etc.\n",
    "2. **Feature Engineering**: Creating new features from existing data to improve model performance.\n",
    "3. **Model Selection**: Choosing the best model from a wide range of algorithms.\n",
    "4. **Hyperparameter Tuning**: Optimizing the parameters of the chosen model to improve performance.\n",
    "5. **Model Evaluation**: Assessing the performance of the model using various metrics.\n",
    "6. **Model Deployment**: Making the model available for use in production.\n",
    "\n",
    "AutoML tools aim to make machine learning accessible to non-experts and improve the efficiency of experts by automating repetitive tasks and providing state-of-the-art models with minimal effort.\n",
    "\n",
    "Popular AutoML frameworks include:\n",
    "- **Google Cloud AutoML**\n",
    "- **H2O.ai**\n",
    "- **Auto-sklearn**\n",
    "- **TPOT**\n",
    "- **MLBox**\n",
    "\n",
    "In this notebook, we will explore how to use one of these AutoML frameworks to build and evaluate a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Sklearn\n",
    "\n",
    "Auto-Sklearn is an open-source AutoML tool built on top of the popular scikit-learn library. It automates the process of model selection, hyperparameter tuning, and ensemble construction. \n",
    "\n",
    "Auto-Sklearn leverages **Bayesian optimization** to find the best model and hyperparameters for a given dataset.\n",
    "\n",
    "How Auto-Sklearn Works\n",
    "\n",
    "Auto-sklearn automates the following:\n",
    "1. Model Selection – Tries multiple ML models (e.g., Decision Trees, Random Forests, SVMs).\n",
    "2. Hyperparameter Optimization – Uses Bayesian Optimization to tune hyperparameters.\n",
    "3. Feature Engineering & Preprocessing – Automatically applies transformations like normalization, one-hot encoding, and missing value imputation.\n",
    "4. Meta-Learning – Uses knowledge from past datasets to speed up training.\n",
    "5. Ensembling – Combines multiple models to improve performance.\n",
    "\n",
    "It builds on top of Scikit-Learn and is particularly useful for tabular data (classification and regression tasks).\n",
    "\n",
    "In the following sections, we will demonstrate how to use Auto-Sklearn to build and evaluate a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How Auto-Sklearn Differs from Grid Search or Random Search**\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Feature</th>\n",
    "        <th>Grid Search</th>\n",
    "        <th>Random Search</th>\n",
    "        <th>Auto-Sklearn</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Model Selection</td>\n",
    "        <td>Manual</td>\n",
    "        <td>Manual</td>\n",
    "        <td>Automatic</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Hyperparameter Tuning</td>\n",
    "        <td>Exhaustive</td>\n",
    "        <td>Random Sampling</td>\n",
    "        <td>Bayesian Optimization</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Preprocessing</td>\n",
    "        <td>Manual</td>\n",
    "        <td>Manual</td>\n",
    "        <td>Automatic</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Uses Prior Knowledge</td>\n",
    "        <td>❌ No</td>\n",
    "        <td>❌ No</td>\n",
    "        <td>✅ Yes (Meta-Learning)</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-sklearn\n",
      "  Downloading auto-sklearn-0.15.0.tar.gz (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m351.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/purus15987/anaconda3/lib/python3.10/site-packages (1.1.1)\n",
      "Collecting distro\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/purus15987/anaconda3/lib/python3.10/site-packages (from auto-sklearn) (1.10.0)\n",
      "Requirement already satisfied: distributed>=2012.12 in /Users/purus15987/anaconda3/lib/python3.10/site-packages (from auto-sklearn) (2022.7.0)\n",
      "Collecting liac-arff\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /Users/purus15987/anaconda3/lib/python3.10/site-packages (from auto-sklearn) (1.23.5)\n",
      "Collecting smac<1.3,>=1.2\n",
      "  Downloading smac-1.2.tar.gz (260 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m278.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /Users/purus15987/anaconda3/lib/python3.10/site-packages (from auto-sklearn) (6.0)\n",
      "Collecting pyrfr<0.9,>=0.8.1\n",
      "  Downloading pyrfr-0.8.3.tar.gz (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m515.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/purus15987/anaconda3/lib/python3.10/site-packages (from auto-sklearn) (65.6.3)\n",
      "Requirement already satisfied: pandas>=1.0 in /Users/purus15987/anaconda3/lib/python3.10/site-packages (from auto-sklearn) (1.5.3)\n",
      "Collecting pynisher<0.7,>=0.6.3\n",
      "  Downloading pynisher-0.6.4.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: dask>=2021.12 in /Users/purus15987/anaconda3/lib/python3.10/site-packages (from auto-sklearn) (2022.7.0)\n",
      "Collecting scikit-learn<0.25.0,>=0.24.0\n",
      "  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m365.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[23 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m <string>:17: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  \u001b[31m   \u001b[0m Partial import of sklearn during the build process.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/purus15987/anaconda3/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 351, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/purus15987/anaconda3/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 333, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/purus15987/anaconda3/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/88/6nnr3fz97_x1_gxvykw8kyzm0000gn/T/pip-build-env-rez78xi4/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/88/6nnr3fz97_x1_gxvykw8kyzm0000gn/T/pip-build-env-rez78xi4/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 304, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/88/6nnr3fz97_x1_gxvykw8kyzm0000gn/T/pip-build-env-rez78xi4/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 522, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/88/6nnr3fz97_x1_gxvykw8kyzm0000gn/T/pip-build-env-rez78xi4/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 320, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 116, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/88/6nnr3fz97_x1_gxvykw8kyzm0000gn/T/pip-build-env-rez78xi4/overlay/lib/python3.10/site-packages/numpy/distutils/__init__.py\", line 26, in <module>\n",
      "  \u001b[31m   \u001b[0m     from . import ccompiler\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/88/6nnr3fz97_x1_gxvykw8kyzm0000gn/T/pip-build-env-rez78xi4/overlay/lib/python3.10/site-packages/numpy/distutils/ccompiler.py\", line 733, in <module>\n",
      "  \u001b[31m   \u001b[0m     ccompiler._default_compilers += (('linux.*', 'intel'),\n",
      "  \u001b[31m   \u001b[0m AttributeError: module 'distutils.ccompiler' has no attribute '_default_compilers'. Did you mean: 'get_default_compiler'?\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install auto-sklearn joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Sklearn for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end-to-end process of using Auto-Sklearn for different tasks such as classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Task: Data Preparation**\n",
    "\n",
    "We load the Iris dataset, which is a popular dataset for classification tasks. The dataset is then split into training and testing sets using an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autosklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautosklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoSklearnClassifier\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autosklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autosklearn.classification import AutoSklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Task: Model Training**\n",
    "\n",
    "**What Happens Behind the Scenes?**\n",
    "- Auto-Sklearn tries multiple models (e.g., Random Forest, SVM, Gradient Boosting).\n",
    "- It optimizes hyperparameters automatically.\n",
    "- The best models are ensembled to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AutoSklearnClassifier\n",
    "automl = AutoSklearnClassifier(\n",
    "    time_left_for_this_task=300,  # Run AutoML for 5 minutes\n",
    "    per_run_time_limit=30,  # Max time per model\n",
    "    ensemble_size=10  # Number of models in the final ensemble\n",
    ")\n",
    "# Train the model\n",
    "automl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Task: Model Evaluation**\n",
    "\n",
    "The trained model is used to make predictions on the test data. We then calculate the accuracy of the model by comparing the predicted labels with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = automl.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Classification Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Sklearn for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the Boston housing dataset and AutoSklearnRegressor from the autosklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from autosklearn.regression import AutoSklearnRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Task: Data Preparation**\n",
    "We load the Boston housing dataset, which is commonly used for regression tasks. The dataset is then split into training and testing sets using an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Task: Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AutoSklearnRegressor\n",
    "automl = AutoSklearnRegressor(time_left_for_this_task=120, per_run_time_limit=30)\n",
    "\n",
    "# Train the model\n",
    "automl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Task: Model Evaluation**\n",
    "\n",
    "The trained model is used to make predictions on the test data. We then calculate the mean squared error (MSE) of the model by comparing the predicted values with the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = automl.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(f'Regression Mean Squared Error: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Sklearn's Features in autosklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Auto-Sklearn’s Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Best Models Used -  Details about the best models found.\n",
    "print(automl.show_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Leaderboard - Ranked list of best models.\n",
    "print(automl.leaderboard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pipeline - Best preprocessing + model pipeline\n",
    "print(automl.get_pipeline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Best Pipeline\n",
    "print(automl.show_best_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final ensemble\n",
    "models = automl.get_models_with_weights()\n",
    "for weight, model in models:\n",
    "    print(f\"Weight: {weight}, Model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results - Performance metrics for all tried models.\n",
    "print(automl.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the statistics -  General summary of the AutoML process.\n",
    "#  Shows details like the number of models tried, total time taken, and the best score.\n",
    "print(automl.sprint_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and Loading Auto-Sklearn Models\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(automl, \"automl_model.pkl\")\n",
    "\n",
    "# Load the model\n",
    "automl = joblib.load(\"automl_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
