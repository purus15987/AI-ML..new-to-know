{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a Recurrent Neural Network (RNN)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Recurrent Neural Network (RNN) is a type of artificial neural network designed for processing sequences of data. \n",
    "\n",
    "Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, allowing them to maintain a hidden state that can capture information about previous inputs in the sequence. \n",
    "\n",
    "This makes RNNs particularly well-suited for tasks where the order of the data is important, such as \n",
    "\n",
    "- Time-Series Prediction (e.g., stock prices, weather forecasting)\n",
    "- Natural Language Processing (NLP) (e.g., machine translation, sentiment analysis)\n",
    "- Speech Recognition (e.g., voice assistants)\n",
    "- Music Generation (e.g., composing melodies)\n",
    "\n",
    "Key characteristics of RNNs include:\n",
    "- **Sequential Data Processing**: \n",
    "    - RNNs process input data in a sequential manner, one element at a time, while maintaining a hidden state that carries information from previous time steps.\n",
    "    - Processes input data in a time-dependent manner.\n",
    "- **Memory of Past Inputs**: \n",
    "    - Uses hidden states to retain past information.\n",
    "- **Weight Sharing**:\n",
    "    - The same weights are used at every time step, reducing the number of trainable parameters.\n",
    "- **Backpropagation Through Time (BPTT)**: \n",
    "    - Training RNNs involves a variant of the backpropagation algorithm called Backpropagation Through Time, which accounts for the sequential nature of the data.\n",
    "\n",
    "However, standard RNNs can struggle with learning long-term dependencies due to issues like vanishing and exploding gradients. To address these challenges, more advanced variants such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) have been developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differences Between Feedforward Networks and RNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Feature                        | Feedforward Networks                          | Recurrent Neural Networks (RNNs)               |\n",
    "|--------------------------------|-----------------------------------------------|------------------------------------------------|\n",
    "| Data Processing                | Processes input data all at once              | Processes input data sequentially              |\n",
    "| Memory                         | No memory of past inputs                      | Maintains hidden state to remember past inputs |\n",
    "| Weight Sharing                 | Different weights for each layer              | Same weights used at every time step           |\n",
    "| Suitable for                   | Static data (e.g., images)                    | Sequential data (e.g., time series, text)      |\n",
    "| Training Algorithm             | Standard backpropagation                      | Backpropagation Through Time (BPTT)            |\n",
    "| Long-Term Dependencies         | Not suitable                                  | Can struggle, but improved with LSTM/GRU       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core Concepts of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Hidden State**: \n",
    "    - The hidden state is a key component of RNNs that allows them to maintain information about previous inputs in the sequence. \n",
    "    - It is updated at each time step based on the current input and the previous hidden state.\n",
    "\n",
    "2. **Input Sequence**: \n",
    "    - RNNs process data as a sequence of inputs, where each element in the sequence is fed into the network one at a time. \n",
    "    - The order of the inputs is crucial for capturing temporal dependencies.\n",
    "\n",
    "3. **Output Sequence**: \n",
    "    - Depending on the task, RNNs can produce an output at each time step or only at the final time step. \n",
    "    - The output can be a sequence of predictions or a single prediction based on the entire input sequence.\n",
    "\n",
    "4. **Weights and Biases**: \n",
    "    - RNNs use the same set of weights and biases at each time step, which allows them to generalize across different positions in the sequence. \n",
    "    - This weight sharing reduces the number of parameters and helps in learning temporal patterns.\n",
    "\n",
    "5. **Activation Function**: \n",
    "    - The activation function (e.g., tanh, ReLU) is applied to the hidden state to introduce non-linearity into the model, enabling it to learn complex patterns in the data.\n",
    "\n",
    "6. **Backpropagation Through Time (BPTT)**: \n",
    "    - BPTT is a variant of the backpropagation algorithm used to train RNNs. \n",
    "    - It involves unrolling the network through time and computing gradients for each time step, which are then used to update the weights and biases.\n",
    "\n",
    "7. **Long-Term Dependencies**: \n",
    "    - Standard RNNs can struggle with learning long-term dependencies due to issues like vanishing and exploding gradients. Advanced variants like LSTM and GRU are designed to address these challenges by incorporating gating mechanisms that regulate the flow of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Recurrent Neurons**:\n",
    "- Recurrent neurons are the fundamental building blocks of RNNs.\n",
    "- Unlike traditional neurons, recurrent neurons have connections that loop back to themselves, allowing them to maintain a hidden state that captures information from previous time steps.\n",
    "- This feedback loop enables the network to process sequences of data and learn temporal dependencies.\n",
    "\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20241030134529497963/recurrent-neuron.png' alt='recuurent_neuron'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**RNN Unfolding**:\n",
    "- RNN unfolding refers to the process of unrolling the recurrent network through time.\n",
    "- During training, the RNN is unfolded to create a sequence of copies of the network, one for each time step in the input sequence.\n",
    "- Each copy shares the same weights and biases but processes a different element of the input sequence.\n",
    "- The hidden state is passed from one time step to the next, allowing the network to maintain information about previous inputs.\n",
    "- Unfolding the RNN is essential for applying the Backpropagation Through Time (BPTT) algorithm, which computes gradients for each time step and updates the weights accordingly.\n",
    "\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20231204131012/Unfolding-660.png' alt='recuurent_neuron'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How RNN works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Layer\n",
    "\n",
    "- Receives sequential input data X = (x1, x2,...,xT).\n",
    "- Each xt represents an element in the sequence at time step t.\n",
    "\n",
    "In RNNs, the hidden state \\( H_i \\) is calculated for every input \\( X_i \\) to retain sequential dependencies. The computations follow these core formulas:\n",
    "\n",
    "1. **Hidden State Update**:\n",
    "\\[ H_i = f(W_{hx} X_i + W_{hh} H_{i-1} + b_h) \\]\n",
    "    - \\( W_{hx} \\): Weight matrix for input to hidden state\n",
    "    - \\( W_{hh} \\): Weight matrix for hidden state to hidden state\n",
    "    - \\( b_h \\): Bias term\n",
    "    - \\( f \\): Activation function (e.g., tanh, ReLU)\n",
    "\n",
    "2. **Output Calculation**:\n",
    "\\[ O_i = g(W_{ho} H_i + b_o) \\]\n",
    "    - \\( W_{ho} \\): Weight matrix for hidden state to output\n",
    "    - \\( b_o \\): Bias term\n",
    "    - \\( g \\): Activation function for output (e.g., softmax for classification tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation in RNNs involves processing an input sequence step-by-step while maintaining a hidden state that carries past information. \n",
    "\n",
    "- At time step t, the RNN takes input xt and previous hidden state htâˆ’1.\n",
    "- Computes the new hidden state ht.\n",
    "- Generates an output yt.\n",
    "- Repeats for all time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Hidden State:\n",
      " [[ 0.95516432]\n",
      " [-0.99182894]\n",
      " [-0.03554066]\n",
      " [-0.99213409]\n",
      " [-0.12484929]]\n",
      "Outputs at each time step:\n",
      " [array([[ 2.70878155],\n",
      "       [-0.35994001]]), array([[ 0.41051732],\n",
      "       [-3.81326106]]), array([[1.45961374],\n",
      "       [1.14008039]]), array([[ 1.95410691],\n",
      "       [-1.2282841 ]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize RNN parameters\n",
    "input_size = 3   # Input vector size\n",
    "hidden_size = 5  # Hidden state size\n",
    "output_size = 2  # Output size\n",
    "seq_length = 4   # Number of time steps\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "np.random.seed(42)\n",
    "W_x = np.random.randn(hidden_size, input_size)   # Input to hidden weights\n",
    "W_h = np.random.randn(hidden_size, hidden_size)  # Hidden to hidden weights\n",
    "W_y = np.random.randn(output_size, hidden_size)  # Hidden to output weights\n",
    "b_h = np.random.randn(hidden_size, 1)            # Bias for hidden state\n",
    "b_y = np.random.randn(output_size, 1)            # Bias for output\n",
    "\n",
    "# Input sequence (4 time steps, batch size = 1)\n",
    "X = np.random.randn(seq_length, input_size, 1)\n",
    "\n",
    "# Initialize hidden state\n",
    "h_t = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward propagation\n",
    "outputs = []\n",
    "for t in range(seq_length):\n",
    "    h_t = np.tanh(np.dot(W_x, X[t]) + np.dot(W_h, h_t) + b_h)  # Hidden state update\n",
    "    y_t = np.dot(W_y, h_t) + b_y  # Output calculation\n",
    "    outputs.append(y_t)\n",
    "\n",
    "# Print results\n",
    "print(\"Final Hidden State:\\n\", h_t)\n",
    "print(\"Outputs at each time step:\\n\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom RNN Cell Implementation (NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Hidden State:\n",
      " [[ 0.95516432]\n",
      " [-0.99182894]\n",
      " [-0.03554066]\n",
      " [-0.99213409]\n",
      " [-0.12484929]]\n",
      "Outputs at each time step:\n",
      " [[[ 2.70878155]\n",
      "  [-0.35994001]]\n",
      "\n",
      " [[ 0.41051732]\n",
      "  [-3.81326106]]\n",
      "\n",
      " [[ 1.45961374]\n",
      "  [ 1.14008039]]\n",
      "\n",
      " [[ 1.95410691]\n",
      "  [-1.2282841 ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size)   # Input to hidden weights\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size)  # Hidden to hidden weights\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size)  # Hidden to output weights\n",
    "        self.b_h = np.random.randn(hidden_size, 1)            # Bias for hidden state\n",
    "        self.b_y = np.random.randn(output_size, 1)            # Bias for output\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: Input sequence of shape (seq_length, input_size, 1)\n",
    "        \"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        h_t = np.zeros((self.hidden_size, 1))  # Initialize hidden state\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            h_t = np.tanh(np.dot(self.W_xh, X[t]) + np.dot(self.W_hh, h_t) + self.b_h)  # Update hidden state\n",
    "            y_t = np.dot(self.W_hy, h_t) + self.b_y  # Compute output\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        return np.array(outputs), h_t  # Return outputs and final hidden state\n",
    "\n",
    "# Define input, hidden, and output sizes\n",
    "input_size = 3\n",
    "hidden_size = 5\n",
    "output_size = 2\n",
    "seq_length = 4\n",
    "\n",
    "# Initialize RNN\n",
    "rnn = CustomRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Create a sample input sequence\n",
    "X = np.random.randn(seq_length, input_size, 1)\n",
    "\n",
    "# Perform forward pass\n",
    "outputs, final_hidden = rnn.forward(X)\n",
    "\n",
    "print(\"Final Hidden State:\\n\", final_hidden)\n",
    "print(\"Outputs at each time step:\\n\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation Through Time (BPTT) in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During forward propagation, an RNN processes a sequence step by step, maintaining hidden states.\n",
    "\n",
    "During backward propagation, errors from the output layer are propagated backward through time to update the weights.\n",
    "- Loss Function (e.g., Mean Squared Error for regression, Cross-Entropy for classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation Through Time (BPTT) is an extension of the backpropagation algorithm for training recurrent neural networks (RNNs). Here's a brief explanation of the gradient calculation in BPTT:\n",
    "\n",
    "**1. Output Layer Gradient**\n",
    "    - The gradient of the loss with respect to the output layer is calculated first. This is similar to the standard backpropagation in feedforward neural networks.\n",
    "\n",
    "        # Assuming L is the loss, y is the output, and y_hat is the predicted output\n",
    "        dL_dy_hat = y_hat - y\n",
    "\n",
    "\n",
    "\n",
    "**2. Hidden State Gradient (Recursive)**\n",
    "    - The gradient of the loss with respect to the hidden state is calculated recursively. This involves propagating the gradients backward through time.\n",
    "\n",
    "        # Assuming h_t is the hidden state at time t, and W_hy is the weight matrix from hidden state to output\n",
    "        dL_dh_t = dL_dy_hat @ W_hy.T\n",
    "\n",
    "        # Recursively calculate the gradient for previous hidden states\n",
    "        for t in reversed(range(T)):\n",
    "            dL_dh_t += dL_dh_t_plus_1 @ W_hh.T * (1 - h_t ** 2)  # Assuming tanh activation\n",
    "            dL_dW_hh += dL_dh_t @ h_t_minus_1.T\n",
    "            dL_db_h += dL_dh_t\n",
    "\n",
    "\n",
    "\n",
    "**3. Weight Update (Gradient Descent Rule)**\n",
    "    - Finally, the weights are updated using the gradients calculated above. This is done using the gradient descent rule.\n",
    "\n",
    "        # Assuming learning_rate is the learning rate\n",
    "        W_hy -= learning_rate * dL_dW_hy\n",
    "        W_hh -= learning_rate * dL_dW_hh\n",
    "        b_h -= learning_rate * dL_db_h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.5586\n",
      "Epoch 10, Loss: 1.6068\n",
      "Epoch 20, Loss: 1.0345\n",
      "Epoch 30, Loss: 0.7091\n",
      "Epoch 40, Loss: 0.4858\n",
      "Epoch 50, Loss: 0.3457\n",
      "Epoch 60, Loss: 0.2433\n",
      "Epoch 70, Loss: 0.1678\n",
      "Epoch 80, Loss: 0.1156\n",
      "Epoch 90, Loss: 0.1289\n",
      "Epoch 100, Loss: 0.1353\n",
      "Epoch 110, Loss: 0.1073\n",
      "Epoch 120, Loss: 0.0860\n",
      "Epoch 130, Loss: 0.0688\n",
      "Epoch 140, Loss: 0.0548\n",
      "Epoch 150, Loss: 0.0436\n",
      "Epoch 160, Loss: 0.0347\n",
      "Epoch 170, Loss: 0.0277\n",
      "Epoch 180, Loss: 0.0221\n",
      "Epoch 190, Loss: 0.0177\n",
      "Epoch 200, Loss: 0.0142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "        np.random.seed(42)\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size)\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size)\n",
    "        self.W_hy = np.random.randn(output_size, hidden_size)\n",
    "        self.b_h = np.random.randn(hidden_size, 1)\n",
    "        self.b_y = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" Forward pass through time \"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "        self.h_states = [np.zeros((self.hidden_size, 1))]  # Store hidden states\n",
    "        self.outputs = []\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            h_t = np.tanh(np.dot(self.W_xh, X[t]) + np.dot(self.W_hh, self.h_states[-1]) + self.b_h)\n",
    "            y_t = np.dot(self.W_hy, h_t) + self.b_y\n",
    "            self.h_states.append(h_t)\n",
    "            self.outputs.append(y_t)\n",
    "\n",
    "        return np.array(self.outputs)\n",
    "\n",
    "    def backward(self, X, Y, outputs):\n",
    "        \"\"\" Backpropagation Through Time (BPTT) \"\"\"\n",
    "        seq_length = X.shape[0]\n",
    "\n",
    "        # Initialize gradients\n",
    "        dW_xh, dW_hh, dW_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy)\n",
    "        db_h, db_y = np.zeros_like(self.b_h), np.zeros_like(self.b_y)\n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        for t in reversed(range(seq_length)):\n",
    "            dy = outputs[t] - Y[t]  # Error at output\n",
    "            dW_hy += np.dot(dy, self.h_states[t+1].T)\n",
    "            db_y += dy\n",
    "\n",
    "            # Backpropagate into hidden state\n",
    "            dh = np.dot(self.W_hy.T, dy) + dh_next\n",
    "            dh_raw = (1 - self.h_states[t+1]**2) * dh  # tanh derivative\n",
    "            dW_xh += np.dot(dh_raw, X[t].T)\n",
    "            dW_hh += np.dot(dh_raw, self.h_states[t].T)\n",
    "            db_h += dh_raw\n",
    "            dh_next = np.dot(self.W_hh.T, dh_raw)\n",
    "\n",
    "        # Update weights\n",
    "        for param, dparam in zip([self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y], \n",
    "                                 [dW_xh, dW_hh, dW_hy, db_h, db_y]):\n",
    "            param -= self.learning_rate * dparam\n",
    "\n",
    "    def train(self, X, Y, epochs=100):\n",
    "        \"\"\" Train the RNN \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            outputs = self.forward(X)\n",
    "            loss = np.mean((outputs - Y) ** 2)  # Mean Squared Error\n",
    "            self.backward(X, Y, outputs)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Define input, hidden, and output sizes\n",
    "input_size, hidden_size, output_size, seq_length = 3, 5, 2, 4\n",
    "\n",
    "# Initialize RNN\n",
    "rnn = CustomRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Sample input and target\n",
    "X = np.random.randn(seq_length, input_size, 1)\n",
    "Y = np.random.randn(seq_length, output_size, 1)\n",
    "\n",
    "# Train the RNN\n",
    "rnn.train(X, Y, epochs=201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.30921238],\n",
       "        [ 0.33126343],\n",
       "        [ 0.97554513]],\n",
       "\n",
       "       [[-0.47917424],\n",
       "        [-0.18565898],\n",
       "        [-1.10633497]],\n",
       "\n",
       "       [[-1.19620662],\n",
       "        [ 0.81252582],\n",
       "        [ 1.35624003]],\n",
       "\n",
       "       [[-0.07201012],\n",
       "        [ 1.0035329 ],\n",
       "        [ 0.36163603]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.64511975],\n",
       "        [ 0.36139561]],\n",
       "\n",
       "       [[ 1.53803657],\n",
       "        [-0.03582604]],\n",
       "\n",
       "       [[ 1.56464366],\n",
       "        [-2.6197451 ]],\n",
       "\n",
       "       [[ 0.8219025 ],\n",
       "        [ 0.08704707]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = rnn.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.57034909],\n",
       "        [ 0.25712017]],\n",
       "\n",
       "       [[ 1.55243268],\n",
       "        [-0.03963926]],\n",
       "\n",
       "       [[ 1.4533344 ],\n",
       "        [-2.53618708]],\n",
       "\n",
       "       [[ 0.66825881],\n",
       "        [ 0.28576962]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012394317319274125"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((outputs - Y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) face several challenges, including the vanishing gradient problem, the exploding gradient problem, and difficulties with long-term dependencies. Here are explanations and solutions for each:\n",
    "\n",
    "**a. Vanishing Gradient Problem**\n",
    "- During backpropagation, gradients can become very small, causing the weights to update very slowly.\n",
    "- This makes it difficult for the network to learn long-term dependencies.\n",
    "\n",
    "**Solutions:**\n",
    "1. **Long Short-Term Memory (LSTM):**\n",
    "   - LSTMs use gates to control the flow of information and maintain gradients over long sequences.\n",
    "2. **Gated Recurrent Units (GRU):**\n",
    "   - GRUs are a simpler alternative to LSTMs that also help mitigate the vanishing gradient problem.\n",
    "3. **Gradient Clipping:**\n",
    "   - Clipping the gradients during backpropagation to prevent them from becoming too small.\n",
    "\n",
    "**b. Exploding Gradient Problem**\n",
    "- During backpropagation, gradients can become very large, causing the weights to update too much.\n",
    "- This can lead to unstable training and divergence.\n",
    "\n",
    "**Solutions:**\n",
    "1. **Gradient Clipping:**\n",
    "   - Clipping the gradients to a maximum value to prevent them from becoming too large.\n",
    "2. **Weight Regularization:**\n",
    "   - Applying regularization techniques like L2 regularization to keep the weights small.\n",
    "\n",
    "**c. Long-Term Dependencies**\n",
    "- RNNs struggle to learn dependencies that span long sequences due to the vanishing gradient problem.\n",
    "\n",
    "**Solutions:**\n",
    "1. **LSTM and GRU:**\n",
    "   - Both architectures are designed to handle long-term dependencies better than standard RNNs.\n",
    "2. **Attention Mechanisms:**\n",
    "   - Attention mechanisms allow the model to focus on relevant parts of the input sequence, improving the handling of long-term dependencies.\n",
    "3. **Transformer Models:**\n",
    "   - Transformers use self-attention mechanisms to capture long-term dependencies without relying on sequential processing.\n",
    "\n",
    "By addressing these challenges with the appropriate techniques, RNNs can be made more effective for various sequence modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that is well-suited to learning from sequences of data. \n",
    "\n",
    "LSTMs are designed to overcome the limitations of traditional RNNs, particularly the problem of vanishing and exploding gradients. They achieve this by incorporating a `memory cell and three gates`: the input gate, the forget gate, and the output gate.\n",
    "\n",
    "**Components of an LSTM**\n",
    "\n",
    "1. **Cell State**:\n",
    "   - The cell state is the memory of the network. \n",
    "   - It runs through the entire chain, with only some minor linear interactions. \n",
    "   - It allows information to flow unchanged, which helps in preserving long-term dependencies.\n",
    "\n",
    "2. **Input Gate**:\n",
    "   - The input gate decides how much of the new information from the current input and the previous hidden state should be added to the cell state. \n",
    "   - It consists of a sigmoid layer and a tanh layer.\n",
    "   - The sigmoid layer outputs values between 0 and 1, which are multiplied by the tanh layer's output to determine the amount of new information to be added.\n",
    "\n",
    "3. **Forget Gate**:\n",
    "   - The forget gate decides what information should be discarded from the cell state. \n",
    "   - It also consists of a sigmoid layer that outputs values between 0 and 1, which are multiplied by the cell state to determine what to forget.\n",
    "\n",
    "4. **Output Gate**:\n",
    "   - The output gate decides what the next hidden state should be. \n",
    "   - It uses the cell state and the current input to produce the hidden state for the next time step. \n",
    "   - It consists of a sigmoid layer and a tanh layer.\n",
    "   - The sigmoid layer determines which parts of the cell state to output, and the tanh layer scales the cell state to a value between -1 and 1.\n",
    "\n",
    "**LSTM Equations**\n",
    "\n",
    "Here are the equations that define the operations of an LSTM cell:\n",
    "\n",
    "      # Forget gate\n",
    "      f_t = sigmoid(np.dot(W_f, [h_{t-1}, x_t]) + b_f)\n",
    "\n",
    "      # Input gate\n",
    "      i_t = sigmoid(np.dot(W_i, [h_{t-1}, x_t]) + b_i)\n",
    "      CÌƒ_t = tanh(np.dot(W_C, [h_{t-1}, x_t]) + b_C)\n",
    "\n",
    "      # Cell state\n",
    "      C_t = f_t * C_{t-1} + i_t * CÌƒ_t\n",
    "\n",
    "      # Output gate\n",
    "      o_t = sigmoid(np.dot(W_o, [h_{t-1}, x_t]) + b_o)\n",
    "      h_t = o_t * tanh(C_t)\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "- `x_t` is the input at time step `t`.\n",
    "- `h_{t-1}` is the hidden state from the previous time step.\n",
    "- `C_{t-1}` is the cell state from the previous time step.\n",
    "- `W_f`, `W_i`, `W_C`, `W_o` are the weight matrices for the forget gate, input gate, cell state, and output gate, respectively.\n",
    "- `b_f`, `b_i`, `b_C`, `b_o` are the bias terms for the forget gate, input gate, cell state, and output gate, respectively.\n",
    "- `sigmoid` and `tanh` are the activation functions.\n",
    "\n",
    "These components and equations allow LSTMs to maintain and update a memory cell, enabling them to learn long-term dependencies in sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state: [[ 0.10300916]\n",
      " [ 0.03630028]\n",
      " [-0.49784764]\n",
      " [-0.19816366]\n",
      " [-0.08696417]]\n",
      "Cell state: [[ 0.77549715]\n",
      " [ 0.0670822 ]\n",
      " [-0.65920812]\n",
      " [-0.39389873]\n",
      " [-0.67070856]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W_f = np.random.randn(hidden_dim, hidden_dim + input_dim)\n",
    "        self.b_f = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        self.W_i = np.random.randn(hidden_dim, hidden_dim + input_dim)\n",
    "        self.b_i = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        self.W_C = np.random.randn(hidden_dim, hidden_dim + input_dim)\n",
    "        self.b_C = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        self.W_o = np.random.randn(hidden_dim, hidden_dim + input_dim)\n",
    "        self.b_o = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x_t, h_prev, C_prev):\n",
    "        # Concatenate hidden state and input\n",
    "        concat = np.vstack((h_prev, x_t))\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = self.sigmoid(np.dot(self.W_f, concat) + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = self.sigmoid(np.dot(self.W_i, concat) + self.b_i)\n",
    "        CÌƒ_t = self.tanh(np.dot(self.W_C, concat) + self.b_C)\n",
    "        \n",
    "        # Cell state\n",
    "        C_t = f_t * C_prev + i_t * CÌƒ_t\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = self.sigmoid(np.dot(self.W_o, concat) + self.b_o)\n",
    "        h_t = o_t * self.tanh(C_t)\n",
    "        \n",
    "        return h_t, C_t\n",
    "\n",
    "# Example usage\n",
    "input_dim = 3  # Example input dimension\n",
    "hidden_dim = 5  # Example hidden state dimension\n",
    "\n",
    "lstm = LSTMCell(input_dim, hidden_dim)\n",
    "\n",
    "# Example input\n",
    "x_t = np.random.randn(input_dim, 1)\n",
    "h_prev = np.zeros((hidden_dim, 1))\n",
    "C_prev = np.zeros((hidden_dim, 1))\n",
    "\n",
    "h_t, C_t = lstm.forward(x_t, h_prev, C_prev)\n",
    "\n",
    "print(\"Hidden state:\", h_t)\n",
    "print(\"Cell state:\", C_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
